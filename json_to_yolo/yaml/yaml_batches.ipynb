{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import logging\n",
    "import sys\n",
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "# Move two directories up from the current file location\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\")))\n",
    "\n",
    "# Now import the LoggerManager\n",
    "from Research_docs.utils.my_logger_module import LoggerManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Logger handlers initialized. Log file: C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\Research_docs\\logs\\dataset_processing\\dataset_logs\\log_2025-02-20_17-29-36.log\n",
      "üßπ Checking for old logs to clean...\n",
      "‚úÖ Logging initialized for 'dataset_logs' in category 'dataset_processing'. Logs saved in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\Research_docs\\logs\\dataset_processing\\dataset_logs\\log_2025-02-20_17-29-36.log\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize LoggerManager\n",
    "logger = LoggerManager(\"dataset_logs\", category=\"dataset_processing\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define updated category mapping based on new labels\n",
    "CATEGORY_MAPPING = {\n",
    "    \"person\": 0,\n",
    "    \"pedestrian\": 0,  # Merge pedestrian into person\n",
    "    \"rider\": 1,\n",
    "    \"car\": 2,\n",
    "    \"truck\": 3,\n",
    "    \"bus\": 4,\n",
    "    \"train\": 5,\n",
    "    \"motor\": 6,  # Motorcycle\n",
    "    \"motorcycle\": 6,  # Merge motorcycle into motor\n",
    "    \"bike\": 7,  # Bicycle\n",
    "    \"bicycle\": 7,  # Merge bicycle into bike\n",
    "    \"traffic light\": 8,\n",
    "    \"traffic sign\": 9,\n",
    "    \"trailer\": 10,\n",
    "    \"other person\": 11,\n",
    "    \"other vehicle\": 12\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the 'names' list follows correct index order\n",
    "CATEGORY_NAMES = [name for name, index in sorted(CATEGORY_MAPPING.items(), key=lambda item: item[1])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Utility Functions ==========\n",
    "\n",
    "def get_image_files(directory: Path):\n",
    "    \"\"\"Returns a list of image file paths in a given directory.\"\"\"\n",
    "    return [f for f in directory.iterdir() if f.suffix in ['.jpg', '.png']] if directory.exists() else []\n",
    "\n",
    "\n",
    "def write_list_to_file(file_path: Path, data_list: list):\n",
    "    \"\"\"Writes a list of strings to a file, creating it if necessary.\"\"\"\n",
    "    try:\n",
    "        with file_path.open('w', encoding='utf-8') as f:\n",
    "            f.write('\\n'.join(data_list) + '\\n')\n",
    "    except Exception as e:\n",
    "        raise IOError(f\"‚ùå Error writing to {file_path}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "def copy_files(files: list, source_dir: Path, dest_dir: Path):\n",
    "    \"\"\"Copies a list of files from source_dir to dest_dir.\"\"\"\n",
    "    for file in files:\n",
    "        source_path = source_dir / file.name\n",
    "        dest_path = dest_dir / file.name\n",
    "        shutil.copy2(source_path, dest_path)\n",
    "\n",
    "\n",
    "def create_directories_if_not_exist(*dirs):\n",
    "    \"\"\"Creates multiple directories if they don‚Äôt exist.\"\"\"\n",
    "    for directory in dirs:\n",
    "        directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# def log_missing_directory(directory: Path, description: str):\n",
    "#     \"\"\"Logs a warning if a directory is missing.\"\"\"\n",
    "#     if not directory.exists():\n",
    "#         logging.warning(f\"Skipping {description}, directory missing: {directory}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(batch_root: Path, num_batches: int, logger):\n",
    "    \"\"\"Creates batch directories if they don't exist.\"\"\"\n",
    "    logger.logger.info(f\"Creating {num_batches} batch directories under {batch_root}...\")\n",
    "\n",
    "    for i in range(1, num_batches + 1):\n",
    "        batch_path = batch_root / f\"batch_{i}\"\n",
    "        batch_path.mkdir(parents=True, exist_ok=True)\n",
    "        logger.logger.info(f\"‚úÖ Created/Verified batch directory: {batch_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_directory_structure(batch_root: Path, logger):\n",
    "    \"\"\"Scans and saves the directory structure for each batch.\"\"\"\n",
    "    logger.logger.info(\"üìÇ Saving directory structure for all batches...\")\n",
    "\n",
    "    # Ensure batch root exists\n",
    "    batch_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for batch_path in batch_root.iterdir():\n",
    "        if not batch_path.is_dir():\n",
    "            continue  # Skip non-directory files\n",
    "\n",
    "        structure_file = batch_path / \"directory_structure.txt\"\n",
    "\n",
    "        # Define expected files\n",
    "        expected_files = [\"train.txt\", \"val.txt\", \"test.txt\"]\n",
    "        structure_content = [\n",
    "            f\"{file}: Exists\" if (batch_path / file).exists() else f\"{file}: Missing\"\n",
    "            for file in expected_files\n",
    "        ]\n",
    "\n",
    "        # Write structure details to file\n",
    "        try:\n",
    "            write_list_to_file(structure_file, structure_content)\n",
    "            logger.logger.info(f\"‚úÖ directory_structure.txt saved in {batch_path}.\")\n",
    "        except Exception as e:\n",
    "            logger.logger.error(f\"‚ùå Error writing directory structure in {batch_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_splits(batch_root: Path, logger):\n",
    "    \"\"\"Creates or updates train.txt, val.txt, and test.txt files listing image paths.\"\"\"\n",
    "    logger.logger.info(\"üìÇ Generating dataset split files (train.txt, val.txt, test.txt)...\")\n",
    "\n",
    "    # Ensure batch root exists\n",
    "    batch_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Define image root\n",
    "    images_root = batch_root / \"images\"\n",
    "\n",
    "    # Define dataset splits\n",
    "    split_files = {\n",
    "        \"train\": batch_root / \"train.txt\",\n",
    "        \"val\": batch_root / \"val.txt\",\n",
    "        \"test\": batch_root / \"test.txt\",\n",
    "    }\n",
    "\n",
    "    # Get image paths for each split\n",
    "    image_paths = {\n",
    "        split: get_image_files(images_root / split)\n",
    "        for split in [\"train\", \"val\", \"test\"]\n",
    "    }\n",
    "\n",
    "    # Write image paths to respective files\n",
    "    for split, file_path in split_files.items():\n",
    "        try:\n",
    "            write_list_to_file(file_path, [str(img.resolve()) for img in image_paths[split]])\n",
    "            logger.logger.info(f\"‚úÖ {split}.txt updated.\")\n",
    "        except Exception as e:\n",
    "            logger.logger.error(f\"‚ùå Error updating {split}.txt: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def create_mini_dataset(source_root: Path, batch_root: Path, num_images=10, num_batches=5, logger=None):\n",
    "    \"\"\"Creates mini batches inside 'batch_root/batches/' from the full dataset.\"\"\"\n",
    "    logger.logger.info(f\"üìÇ Creating {num_batches} mini batches inside {batch_root / 'batches'}...\")\n",
    "\n",
    "    # Ensure source dataset exists\n",
    "    if not source_root.exists():\n",
    "        logger.logger.error(f\"‚ùå Source directory {source_root} does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Ensure batch root exists\n",
    "    batch_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Define `batches/` directory inside `batch_root`\n",
    "    batches_dir = batch_root / \"batches\"\n",
    "    batches_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Define source directories\n",
    "    images_src = source_root / \"images\"\n",
    "    labels_src = source_root / \"labels\"\n",
    "\n",
    "    for batch_num in range(1, num_batches + 1):\n",
    "        batch_dir = batches_dir / f\"batch_{batch_num}\"\n",
    "        batch_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            src_images = images_src / split\n",
    "            src_labels = labels_src / split\n",
    "            dest_images = batch_dir / \"images\" / split\n",
    "            dest_labels = batch_dir / \"labels\" / split\n",
    "\n",
    "            # Ensure batch directories exist\n",
    "            dest_images.mkdir(parents=True, exist_ok=True)\n",
    "            dest_labels.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # Check if source images exist\n",
    "            if not src_images.exists():\n",
    "                logger.logger.warning(f\"‚ö†Ô∏è Skipping {split} in batch_{batch_num}, images folder missing: {src_images}\")\n",
    "                continue\n",
    "\n",
    "            # Get available images\n",
    "            image_files = [f for f in src_images.iterdir() if f.suffix in ['.jpg', '.png']]\n",
    "            if not image_files:\n",
    "                logger.logger.warning(f\"‚ö†Ô∏è No images found in {src_images}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Randomly select images\n",
    "            selected_images = random.sample(image_files, min(num_images, len(image_files)))\n",
    "\n",
    "            # Copy selected images and corresponding labels\n",
    "            for image in selected_images:\n",
    "                shutil.copy2(image, dest_images / image.name)\n",
    "\n",
    "                # Copy corresponding label if exists\n",
    "                label_path = src_labels / f\"{image.stem}.txt\"\n",
    "                if label_path.exists():\n",
    "                    shutil.copy2(label_path, dest_labels / label_path.name)\n",
    "\n",
    "    logger.logger.info(f\"‚úÖ Mini batches created successfully inside {batches_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "def ensure_data_yaml_exists(batch_root: Path, logger):\n",
    "    \"\"\"Ensures a valid data.yaml file and dataset split files exist in each mini batch directory.\n",
    "    If `train.txt`, `val.txt`, or `test.txt` are missing, they will be created with file names inside the batch.\n",
    "    \"\"\"\n",
    "    logger.logger.info(\"üìÑ Creating/updating data.yaml and dataset split files for all mini batches...\")\n",
    "\n",
    "    # Ensure batch root exists\n",
    "    batch_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Define batches directory inside batch_root\n",
    "    batches_dir = batch_root / \"batches\"\n",
    "\n",
    "    # Ensure batches directory exists\n",
    "    if not batches_dir.exists():\n",
    "        logger.logger.error(f\"‚ùå Batches directory {batches_dir} does not exist. Run `create_mini_dataset` first.\")\n",
    "        return\n",
    "\n",
    "    # Iterate over all mini batches\n",
    "    for batch_dir in batches_dir.iterdir():\n",
    "        if not batch_dir.is_dir():\n",
    "            continue  # Skip non-directory files\n",
    "\n",
    "        data_yaml_path = batch_dir / \"data.yaml\"\n",
    "        images_root = batch_dir / \"images\"\n",
    "        labels_root = batch_dir / \"labels\"\n",
    "\n",
    "        # Ensure the batch directories exist\n",
    "        images_root.mkdir(parents=True, exist_ok=True)\n",
    "        labels_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Define dataset splits\n",
    "        dataset_splits = [\"train\", \"val\", \"test\"]\n",
    "        split_files = {split: batch_dir / f\"{split}.txt\" for split in dataset_splits}\n",
    "\n",
    "        # Generate file lists for train, val, and test\n",
    "        for split, file_path in split_files.items():\n",
    "            split_dir = images_root / split\n",
    "            image_files = [f.name for f in split_dir.iterdir() if f.suffix in ['.jpg', '.png']] if split_dir.exists() else []\n",
    "\n",
    "            # Write image names to the corresponding .txt file\n",
    "            with file_path.open('w', encoding='utf-8') as f:\n",
    "                f.write('\\n'.join(image_files) + '\\n')\n",
    "\n",
    "            logger.logger.info(f\"‚úÖ {split}.txt updated in {batch_dir} with {len(image_files)} images.\")\n",
    "\n",
    "        try:\n",
    "            logger.logger.info(f\"üìÑ Overwriting data.yaml in {batch_dir}...\")\n",
    "\n",
    "            data_yaml_content = {\n",
    "                'path': str(batch_dir.resolve()),  # Root dataset path\n",
    "                'train': \"images/train\",\n",
    "                'val': \"images/val\",\n",
    "                'test': \"images/test\",\n",
    "                'nc': 13,  # Number of classes\n",
    "                'names': [\n",
    "                    \"person\", \"rider\", \"car\", \"truck\", \"bus\", \"train\", \"motorcycle\", \"bicycle\",\n",
    "                    \"traffic light\", \"traffic sign\", \"trailer\", \"other person\", \"other vehicle\"\n",
    "                ]\n",
    "            }\n",
    "\n",
    "            # Write to data.yaml\n",
    "            with data_yaml_path.open('w', encoding='utf-8') as f:\n",
    "                yaml.dump(data_yaml_content, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "            logger.logger.info(f\"‚úÖ data.yaml updated successfully in {batch_dir}.\")\n",
    "        except Exception as e:\n",
    "            logger.logger.error(f\"‚ùå Error updating data.yaml in {batch_dir}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6986.3\n"
     ]
    }
   ],
   "source": [
    "print(69863/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 17:29:36,321 - INFO - üìÇ Creating 10 mini batches inside C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches...\n",
      "2025-02-20 17:29:39,562 - INFO - ‚úÖ Mini batches created successfully inside C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\n",
      "2025-02-20 17:29:39,563 - INFO - üìÑ Creating/updating data.yaml and dataset split files for all mini batches...\n",
      "2025-02-20 17:29:39,565 - INFO - ‚úÖ train.txt updated in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_1 with 10 images.\n",
      "2025-02-20 17:29:39,566 - INFO - ‚úÖ val.txt updated in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_1 with 10 images.\n",
      "2025-02-20 17:29:39,566 - INFO - ‚úÖ test.txt updated in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_1 with 10 images.\n",
      "2025-02-20 17:29:39,567 - INFO - üìÑ Overwriting data.yaml in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_1...\n",
      "2025-02-20 17:29:39,568 - INFO - ‚úÖ data.yaml updated successfully in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_1.\n",
      "2025-02-20 17:29:39,568 - INFO - ‚úÖ train.txt updated in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_10 with 10 images.\n",
      "2025-02-20 17:29:39,570 - INFO - ‚úÖ val.txt updated in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_10 with 10 images.\n",
      "2025-02-20 17:29:39,570 - INFO - ‚úÖ test.txt updated in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_10 with 10 images.\n",
      "2025-02-20 17:29:39,570 - INFO - üìÑ Overwriting data.yaml in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_10...\n",
      "2025-02-20 17:29:39,572 - INFO - ‚úÖ data.yaml updated successfully in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_10.\n",
      "2025-02-20 17:29:39,573 - INFO - ‚úÖ train.txt updated in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_2 with 10 images.\n",
      "2025-02-20 17:29:39,575 - INFO - ‚úÖ val.txt updated in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_2 with 10 images.\n",
      "2025-02-20 17:29:39,575 - INFO - ‚úÖ test.txt updated in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_2 with 10 images.\n",
      "2025-02-20 17:29:39,576 - INFO - üìÑ Overwriting data.yaml in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_2...\n",
      "2025-02-20 17:29:39,577 - INFO - ‚úÖ data.yaml updated successfully in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_2.\n",
      "2025-02-20 17:29:39,578 - INFO - ‚úÖ train.txt updated in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_3 with 10 images.\n",
      "2025-02-20 17:29:39,578 - INFO - ‚úÖ val.txt updated in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_3 with 10 images.\n",
      "2025-02-20 17:29:39,579 - INFO - ‚úÖ test.txt updated in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_3 with 10 images.\n",
      "2025-02-20 17:29:39,579 - INFO - üìÑ Overwriting data.yaml in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_3...\n",
      "2025-02-20 17:29:39,580 - INFO - ‚úÖ data.yaml updated successfully in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_3.\n",
      "2025-02-20 17:29:39,582 - INFO - ‚úÖ train.txt updated in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_4 with 10 images.\n",
      "2025-02-20 17:29:39,583 - INFO - ‚úÖ val.txt updated in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_4 with 10 images.\n",
      "2025-02-20 17:29:39,583 - INFO - ‚úÖ test.txt updated in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_4 with 10 images.\n",
      "2025-02-20 17:29:39,584 - INFO - üìÑ Overwriting data.yaml in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_4...\n",
      "2025-02-20 17:29:39,585 - INFO - ‚úÖ data.yaml updated successfully in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_4.\n",
      "2025-02-20 17:29:39,585 - INFO - ‚úÖ train.txt updated in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_5 with 10 images.\n",
      "2025-02-20 17:29:39,586 - INFO - ‚úÖ val.txt updated in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_5 with 10 images.\n",
      "2025-02-20 17:29:39,587 - INFO - ‚úÖ test.txt updated in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_5 with 10 images.\n",
      "2025-02-20 17:29:39,588 - INFO - üìÑ Overwriting data.yaml in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_5...\n",
      "2025-02-20 17:29:39,589 - INFO - ‚úÖ data.yaml updated successfully in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_5.\n",
      "2025-02-20 17:29:39,590 - INFO - ‚úÖ train.txt updated in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_6 with 10 images.\n",
      "2025-02-20 17:29:39,590 - INFO - ‚úÖ val.txt updated in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_6 with 10 images.\n",
      "2025-02-20 17:29:39,591 - INFO - ‚úÖ test.txt updated in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_6 with 10 images.\n",
      "2025-02-20 17:29:39,591 - INFO - üìÑ Overwriting data.yaml in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_6...\n",
      "2025-02-20 17:29:39,594 - INFO - ‚úÖ data.yaml updated successfully in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_6.\n",
      "2025-02-20 17:29:39,595 - INFO - ‚úÖ train.txt updated in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_7 with 10 images.\n",
      "2025-02-20 17:29:39,596 - INFO - ‚úÖ val.txt updated in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_7 with 10 images.\n",
      "2025-02-20 17:29:39,596 - INFO - ‚úÖ test.txt updated in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_7 with 10 images.\n",
      "2025-02-20 17:29:39,596 - INFO - üìÑ Overwriting data.yaml in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_7...\n",
      "2025-02-20 17:29:39,597 - INFO - ‚úÖ data.yaml updated successfully in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_7.\n",
      "2025-02-20 17:29:39,599 - INFO - ‚úÖ train.txt updated in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_8 with 10 images.\n",
      "2025-02-20 17:29:39,600 - INFO - ‚úÖ val.txt updated in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_8 with 10 images.\n",
      "2025-02-20 17:29:39,600 - INFO - ‚úÖ test.txt updated in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_8 with 10 images.\n",
      "2025-02-20 17:29:39,601 - INFO - üìÑ Overwriting data.yaml in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_8...\n",
      "2025-02-20 17:29:39,602 - INFO - ‚úÖ data.yaml updated successfully in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_8.\n",
      "2025-02-20 17:29:39,603 - INFO - ‚úÖ train.txt updated in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_9 with 10 images.\n",
      "2025-02-20 17:29:39,604 - INFO - ‚úÖ val.txt updated in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_9 with 10 images.\n",
      "2025-02-20 17:29:39,605 - INFO - ‚úÖ test.txt updated in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_9 with 10 images.\n",
      "2025-02-20 17:29:39,606 - INFO - üìÑ Overwriting data.yaml in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_9...\n",
      "2025-02-20 17:29:39,607 - INFO - ‚úÖ data.yaml updated successfully in C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\\batches\\batch_9.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Logger 'dataset_logs' closed successfully.\n"
     ]
    }
   ],
   "source": [
    "# üîπ 1Ô∏è‚É£ Initialize Logger\n",
    "logger = LoggerManager(\"dataset_logs\", category=\"dataset_processing\")\n",
    "# Define source and destination paths\n",
    "source_root = Path(r\"C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\data\\bdd100k\")\n",
    "# dest_root = Path(r\"C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\my-project\\data\\bdd100_mini\")\n",
    "\n",
    "batch_root = Path(r\"C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\")\n",
    "\n",
    "# üîπ 3Ô∏è‚É£ Ensure batch root exists\n",
    "batch_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# üîπ 4Ô∏è‚É£ First, save directory structure\n",
    "# save_directory_structure(batch_root, logger)\n",
    "\n",
    "# üîπ 5Ô∏è‚É£ Then, create batch directories (if they don‚Äôt exist)\n",
    "# create_batches(batch_root, num_batches=5, logger=logger)  # Creating 5 batches as an example\n",
    "\n",
    "# create_mini_dataset(source_root, batch_root, num_images=10, num_batches=5, logger=logger)\n",
    "create_mini_dataset(source_root, batch_root, num_images=, num_batches=10, logger=logger)\n",
    "# create_mini_dataset(source_root, num_images=10, num_batches=5, logger=logger)\n",
    "\n",
    "# # üîπ 7Ô∏è‚É£ Save `data.yaml` files inside each batch\n",
    "ensure_data_yaml_exists(batch_root, logger)\n",
    "\n",
    "# # üîπ 8Ô∏è‚É£ Final batch processing check\n",
    "# process_batches(batch_root, logger)\n",
    "\n",
    "# üîπ 9Ô∏è‚É£ Close Logger (Optional, ensures logs are saved)\n",
    "logger.close_logger()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Logger 'dataset_logs' closed successfully.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "def create_equal_batches(source_root: Path, batch_root: Path, num_batches: int, logger=None):\n",
    "    \"\"\"\n",
    "    Creates batches with an equal number of images from train, val, and test sets.\n",
    "    \n",
    "    Args:\n",
    "        source_root (Path): Path to the source dataset directory.\n",
    "        batch_root (Path): Path to the directory where batches will be created.\n",
    "        num_batches (int): Number of batches to create.\n",
    "        logger (LoggerManager, optional): Logger for logging messages. Defaults to None.\n",
    "    \"\"\"\n",
    "    logger.logger.info(f\"üìÇ Creating {num_batches} equal batches inside {batch_root / 'batches'}...\")\n",
    "\n",
    "    # Ensure source dataset exists\n",
    "    if not source_root.exists():\n",
    "        logger.logger.error(f\"‚ùå Source directory {source_root} does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Ensure batch root exists\n",
    "    batch_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Define `batches/` directory inside `batch_root`\n",
    "    batches_dir = batch_root / \"batches\"\n",
    "    batches_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Define source directories\n",
    "    images_src = source_root / \"images\"\n",
    "    labels_src = source_root / \"labels\"\n",
    "\n",
    "    # Get all images from train, val, and test sets\n",
    "    train_images = get_image_files(images_src / \"train\")\n",
    "    val_images = get_image_files(images_src / \"val\")\n",
    "    test_images = get_image_files(images_src / \"test\")\n",
    "\n",
    "    # Calculate the number of images per batch for each split\n",
    "    train_per_batch = len(train_images) // num_batches\n",
    "    val_per_batch = len(val_images) // num_batches\n",
    "    test_per_batch = len(test_images) // num_batches\n",
    "\n",
    "    for batch_num in range(1, num_batches + 1):\n",
    "        batch_dir = batches_dir / f\"batch_{batch_num}\"\n",
    "        batch_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Create subdirectories for images and labels\n",
    "        dest_images = batch_dir / \"images\"\n",
    "        dest_labels = batch_dir / \"labels\"\n",
    "        dest_images.mkdir(parents=True, exist_ok=True)\n",
    "        dest_labels.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Randomly select images for each split\n",
    "        selected_train = random.sample(train_images, train_per_batch)\n",
    "        selected_val = random.sample(val_images, val_per_batch)\n",
    "        selected_test = random.sample(test_images, test_per_batch)\n",
    "\n",
    "        # Copy selected images and corresponding labels\n",
    "        for split, selected_images in zip([\"train\", \"val\", \"test\"], [selected_train, selected_val, selected_test]):\n",
    "            split_images = dest_images / split\n",
    "            split_labels = dest_labels / split\n",
    "            split_images.mkdir(parents=True, exist_ok=True)\n",
    "            split_labels.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            for image in selected_images:\n",
    "                shutil.copy2(image, split_images / image.name)\n",
    "\n",
    "                # Copy corresponding label if exists\n",
    "                label_path = labels_src / split / f\"{image.stem}.txt\"\n",
    "                if label_path.exists():\n",
    "                    shutil.copy2(label_path, split_labels / label_path.name)\n",
    "\n",
    "        logger.logger.info(f\"‚úÖ Batch {batch_num} created with {train_per_batch} train, {val_per_batch} val, and {test_per_batch} test images.\")\n",
    "\n",
    "    logger.logger.info(f\"‚úÖ All {num_batches} batches created successfully inside {batches_dir}\")\n",
    "\n",
    "# Example usage\n",
    "source_root = Path(r\"C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\data\\bdd100k\")\n",
    "batch_root = Path(r\"C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\test\\bdd100_mini\")\n",
    "num_batches = 10  # Number of batches to create\n",
    "\n",
    "# Initialize Logger\n",
    "logger = LoggerManager(\"dataset_logs\", category=\"dataset_processing\")\n",
    "\n",
    "# Create equal batches\n",
    "create_equal_batches(source_root, batch_root, num_batches, logger)\n",
    "\n",
    "# Ensure data.yaml files exist in each batch\n",
    "ensure_data_yaml_exists(batch_root, logger)\n",
    "\n",
    "# Close Logger\n",
    "logger.close_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flower2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
